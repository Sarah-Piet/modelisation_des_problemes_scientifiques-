{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6><i>Classification de chiffres</i></font>\n",
    "\n",
    "Beaucoup de jeux de données sont de très grandes tailles, et faire des calculs dessus nécessite énormément de temps. Certains algorithmes sont décomposables en opérations élementaires, ce qui facilite la gestion des calculs assez lourds.\n",
    "\n",
    "Le but du TP est de voir quelques méthodes qui simplifient les grands calculs sur des grosses bases de données.\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour montrer l'intérêt de la parallélisation, il faut d'abord télécharger un grand jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drone/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 11s 1us/step\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# tensorflow est une librairie qui est assez connue pour l'apprentissage statistique, notamment les réseaux de neurones\n",
    "# on l'utilisera juste pour charger les données\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce jeu de données est une base de données d'images représentant des chiffres de 1 à 9 contenue dans x, accompagné d'une série de labels de \"0\" à \"9\" contenus dans y.\n",
    "\n",
    "x_train[1] contient la première image sous forme de pixels (1 image = 28 pixels*28 pixels), y_train[1] le libellé du premier chiffre.\n",
    "\n",
    "<b>1) Pour s'en convaincre, affichez les premières images à l'aide de la fonction imshow() de matplotlib</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oui, mais dans cette base de données énorme, chaque calcul prend du temps. On va essayer de quantifier ce temps.\n",
    "\n",
    "<b>2) Par exemple, cherchez la probabilité de tomber sur le chiffre 1 dans y_train, et calculez le temps pris par votre calcul à  l'aide de la fonction clock() de la librairie time.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3) Cette instruction tourne plutôt vite. Comment se comporte le temps quand on fait une grosse opération? Faites une analyse en composantes principales avec le package PCA, et affichez les points à l'aide de matplotlib. Il peut être utile de formater les données dans un premier temps, de sorte qu'une image soit écrite dans une seule ligne dans votre base de données, et pas dans un tableau de 28*28. Pour cela, pensez à utiliser la fonction reshape de numpy.\n",
    "\n",
    "Si vous vous sentez à l'aise: tracer des ellipses autour des groupes de points pour \"marquer les limites\" des groupes.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Maintenant, on va comparer le temps d'éxécution d'une opération lancée sur un coeur , au temps d'exécution de la même opération répartie entre plusieurs coeurs. Pour cela , on va utiliser la fonction K-means, vu dans le tp précédent, qui fait de la classification des individus (ici de la classification des chiffres).\n",
    "\n",
    "Dans la logique, il faudrait penser à faire 10 groupes de points, pour que chaque groupe isolé par les kmeans correspondent à un chiffre. Prenez le réflexe d'aller fouiller dans la doc, en l'occurence dans ce cas-là: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Cette fonction dipose d'une option n_jobs qui permet de partager les calculs entre plusieurs coeurs. A priori, le nombre maximal pour n_jobs est le nombre de coeurs que possède votre ordinateur. Pour déterminer le nombre de coeurs:\n",
    "- https://www.it-connect.fr/afficher-le-nombre-de-coeurs-sous-linux/ pour linux \n",
    "- https://support.microsoft.com/fr-fr/help/4026757/windows-10-find-out-how-many-cores-your-processor-has pour windows\n",
    "\n",
    "Testez un algorithme de KMeans lancé avec l'option n_jobs = 1, puis le même algo avec n_jobs = votre nombre de coeurs, et comparez les temps d'exécution.\n",
    "\n",
    "\n",
    "Je vous conseille de lancer un htop depuis linux pour voir comment les coeurs sont utilisés par l'algorithme suivant les différentes configurations (ou le gestionnaire des tâches sous windows).</b>\n",
    "\n",
    "# Attention:\n",
    "Ne prenez que quelques milliers de ligne pour démarrer, sinon l'algorithme prendra trop de temps (10 000 max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 5) Si vous avez le temps, essayez de faire varier n_jobs et et le nombre de lignes. Puis représentez graphiquement tous les temps d'exécution sur un même graphe, en fonction de n_jobs.\n",
    "    \n",
    "Sinon, passez directement à la question 6).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet algorithme est sympathique, au sens où il gère pour vous toute la partie décomposition des opérations. Si vous voulez répartir les opérations sur plusieurs coeurs, il faut dire manuellement à chaque coeur de quoi il doit s'occuper à l'aide de threads. \n",
    "\n",
    "Voir https://openclassrooms.com/fr/courses/235344-apprenez-a-programmer-en-python/2235545-la-programmation-parallele-avec-threading pour aller plus loin sur le sujet.\n",
    "\n",
    "Dans la suite, nous allons nous contenter de décomposer quelques opérations très simples à l'aide du mapreduce.\n",
    "\n",
    "Voir https://nyu-cds.github.io/python-bigdata/02-mapreduce/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 6) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
