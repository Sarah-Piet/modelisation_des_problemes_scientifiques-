{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6><i>Classification de chiffres et comptage de mots</i></font>\n",
    "\n",
    "Beaucoup de jeux de données sont de très grandes tailles, et faire des calculs dessus nécessite énormément de temps. Certains algorithmes sont décomposables en opérations élementaires, ce qui facilite la gestion des calculs assez lourds.\n",
    "\n",
    "Le but du TP est de voir quelques méthodes qui simplifient les grands calculs sur des grosses bases de données.\n",
    "\n",
    "\n",
    "# A-] Classification de chiffres\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour montrer l'intérêt de la parallélisation, il faut d'abord télécharger un grand jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drone/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 11s 1us/step\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# tensorflow est une librairie qui est assez connue pour l'apprentissage statistique, notamment les réseaux de neurones\n",
    "# on l'utilisera juste pour charger les données\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce jeu de données est une base de données d'images représentant des chiffres de 1 à 9 contenue dans x, accompagné d'une série de labels de \"0\" à \"9\" contenus dans y.\n",
    "\n",
    "x_train[1] contient la première image sous forme de pixels (1 image = 28 pixels*28 pixels), y_train[1] le libellé du premier chiffre.\n",
    "\n",
    "<b>1) Pour s'en convaincre, affichez les premières images à l'aide de la fonction imshow() de matplotlib</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2) Par exemple, cherchez la probabilité de tomber sur le chiffre 1 dans y_train, et calculez le temps pris par votre calcul à  l'aide de la fonction clock() de la librairie time.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3) Cette instruction tourne plutôt vite. Comment se comporte le temps quand on fait une grosse opération? Faites une analyse en composantes principales avec le package PCA, et affichez les points à l'aide de matplotlib.</b>\n",
    "    \n",
    "Il peut être utile de formater les données dans un premier temps, de sorte qu'une image soit écrite dans une seule ligne dans votre base de données, et pas dans un tableau de 28*28. Pour cela, pensez à utiliser la fonction reshape de numpy.\n",
    "\n",
    "Si vous vous sentez à l'aise: tracer des ellipses autour des groupes de points pour \"marquer les limites\" des groupes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Maintenant, on va comparer le temps d'éxécution d'une opération lancée sur un coeur , au temps d'exécution de la même opération répartie entre plusieurs coeurs. Pour cela , on va utiliser la fonction K-means, vu dans le tp précédent, qui fait de la classification des individus (ici de la classification des chiffres).</b>\n",
    "\n",
    "Dans la logique, il faudrait penser à faire 10 groupes de points, pour que chaque groupe isolé par les kmeans correspondent à un chiffre. Prenez le réflexe d'aller fouiller dans la doc, en l'occurence dans ce cas-là: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Cette fonction dipose d'une option n_jobs qui permet de partager les calculs entre plusieurs coeurs. A priori, le nombre maximal pour n_jobs est le nombre de coeurs que possède votre ordinateur. Pour déterminer le nombre de coeurs maximal:\n",
    "- https://www.it-connect.fr/afficher-le-nombre-de-coeurs-sous-linux/ pour linux \n",
    "- https://support.microsoft.com/fr-fr/help/4026757/windows-10-find-out-how-many-cores-your-processor-has pour windows\n",
    "\n",
    "Testez un algorithme de KMeans lancé avec l'option n_jobs = 1, puis le même algo avec n_jobs = votre nombre de coeurs, et comparez les temps d'exécution.\n",
    "\n",
    "\n",
    "Je vous conseille de lancer un htop depuis linux pour voir comment les coeurs sont utilisés par l'algorithme suivant les différentes configurations (ou le gestionnaire des tâches sous windows).\n",
    "\n",
    "Ne prenez que quelques milliers de ligne pour démarrer, sinon l'algorithme prendra trop de temps (10 000 max)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour un seul job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La version à quatre jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 5) Si vous avez le temps, essayez de faire varier n_jobs et et le nombre de lignes. Puis représentez graphiquement tous les temps d'exécution sur un même graphe, en fonction de n_jobs.\n",
    "    \n",
    "Sinon, passez directement au B-].</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet algorithme est sympathique, au sens où il gère pour vous toute la partie décomposition des opérations. Si vous voulez répartir les opérations sur plusieurs coeurs, il faut dire manuellement à chaque coeur de quoi il doit s'occuper à l'aide de threads. \n",
    "\n",
    "Voir https://openclassrooms.com/fr/courses/235344-apprenez-a-programmer-en-python/2235545-la-programmation-parallele-avec-threading si vous souhaitez aller plus loin sur le sujet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Bonus)\n",
    "<b> 6) Introduction au machine learning : Utilisez une regression logistique pour prédire les chiffres représentés sur lessur les 10000 images de test à partir de 5000 images issues de l'échantillon d'apprentissage. Combien avez-vous d'erreurs? </b>\n",
    "\n",
    "On pourra utiliser LogisticRegression, de sklearn.linear_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-] Comptage de mots avec Spark\n",
    "\n",
    "\n",
    "C'est quoi spark? C'est un environnement de calculs distribués. Il permet, à l'image des questions précédentes, de décomposer les gros calculs en une multitude de petits calculs qui s'éxécutent en parallèle.\n",
    "\n",
    "Pour l'installation, qui n'est pas évidente à suivre:\n",
    "http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx2/td_3a_spark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ce code ne doit être exécuté qu'une fois. La variable sc est alors notre interface spark, qui tourne en local.import findspark\n",
    "findspark.init(\"C:\\\\Users\\\\llesoil\\\\Downloads\\\\spark\") # à remplacer par votre SPARK_HOME, l'emplacement de spark\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"TP3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code ne doit être exécuté qu'une fois. La variable sc est alors notre interface spark, qui tourne en local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('la', 1),\n",
       " ('ronds', 2),\n",
       " ('Bretagne', 1),\n",
       " ('les', 1),\n",
       " ('chapeaux', 2),\n",
       " ('vive', 2),\n",
       " ('ils', 2),\n",
       " ('ont', 2),\n",
       " ('bretons', 1),\n",
       " ('des', 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'ils ont des chapeaux ronds '+ 'vive la Bretagne '+'ils ont des chapeaux ronds '+ 'vive les bretons' \n",
    "sc.parallelize(s.split()).map(lambda word: (word, 1)).reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le [schéma](https://github.com/llesoil/modelisation_des_problemes_scientifiques-/blob/master/Aides/mapreduce.png), tiré de ce [tuto](https://nyu-cds.github.io/python-bigdata/02-mapreduce/) explique bien le principe du mapreduce, un algorithme utilisé par spark qui permet de distribuer des calculs.:\n",
    "\n",
    "- On répartit les données entre les différentes machines (s'il y a quatre machines, on peut supposer que chacune s'occupe d'une phrase dans notre cas), c'est le mapping\n",
    "- Chacun récupère les mots de sa phrase à partir de la chaine de caractère (le .split() découpe la chaine suivant les espaces)\n",
    "- Chacun compte ses mots individuellement, c'est le map\n",
    "- Puis on passe à la phase reduce, on additionne tous les comptes des mots associés aux quatre phrases (reducebKey)\n",
    "- Enfin, on peut afficher les résultats du dessous\n",
    "\n",
    "L'intérêt, c'est que <B>toute la partie mapping peut se faire en parallèle</B>. \n",
    "\n",
    "Si vous avez un roman à analyser, ça sera plus rapide si chaque ordinateur s'occupe d'un chapitre, comparé à une analyse séquentielle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loutre: 6\n",
      "mer: 6\n",
      "lutris): 1\n",
      "est: 2\n",
      "des: 6\n",
      "must�lid�s): 1\n",
      "vivant: 1\n",
      "dans: 3\n",
      "le: 3\n",
      "du: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = os.getcwd() + \"//\"+\"loutre_des_mers.txt\" \n",
    "# os.getcwd() donne l'adresse du répertoire courant\n",
    "# Vous pouvez télécharger le fichier loutre_des_mers.txt et mettre l'adresse locale dans la variable path\n",
    "\n",
    "file = sc.textFile(path)\n",
    "# Crée le fichier à partir du chemin\n",
    "\n",
    "output = file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b).collect()\n",
    "\n",
    "# on affiche les dix premiers mots\n",
    "for (word, count) in output[0:10]:\n",
    "        print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code fonctionne également avec un fichier texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) Appliquez le même code au fichier twitter_trump.txt, qui contient des tweets concernant/de Donald Trump pris sur http://www.trumptwitterarchive.com/archive. Quels sont les mots qui reviennent le plus fréquemement? Est-ce Barack Obama ou Hillary Clinton qui est le.a plus cité.e dans ce fichier?</b>\n",
    "\n",
    "Indications:\n",
    "- On pourra utiliser des dictionnaires, la fonction sorted() pourra sans doute vous aider. \n",
    "- Si vous voulez de meilleurs résultats, vous pouvez utiliser la fonction lower() sur chacun des mots pour enlever les majuscules, et le module stop_words pour ne pas tenir compte des mots les plus couramment utilisés en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 2) Estimation de la constante pi à l'aide de Spark<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
